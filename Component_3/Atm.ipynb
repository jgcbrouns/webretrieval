{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import codecs\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import gensim\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers = pd.read_csv('/Users/macbook/Master--Y2/IR/nips-papers/papers.csv')\n",
    "\n",
    "#author with unique id\n",
    "authors = pd.read_csv('/Users/macbook/Master--Y2/IR/nips-papers/authors.csv')\n",
    "#paper id + author id\n",
    "paper_author = pd.read_csv('/Users/macbook/Master--Y2/IR/nips-papers/paper_authors.csv')\n",
    "\n",
    "#papers = papers.sort['year']\n",
    "paper_text = papers['paper_text']\n",
    "paper_title = papers['title']\n",
    "del paper_author['id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create author2doc\n",
    "author2doc = dict()\n",
    "for row_index,row in authors.iterrows():\n",
    "    ids =[]\n",
    "    author_id = row['id']\n",
    "    author_name = row['name']\n",
    "    #author_name = re.sub('\\s', '', author_name_1)\n",
    "    for row_index1, row_1 in paper_author.iterrows():\n",
    "        if row_1['author_id'] == author_id:\n",
    "            paper_id = row_1['paper_id']\n",
    "            ids.append(paper_id)\n",
    "    if not author2doc.get(author_name):\n",
    "        # This is a new author.\n",
    "        author2doc[author_name] = []\n",
    "     # Add document IDs to author.\n",
    "    author2doc[author_name].extend([id for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('author2doc.pickle', 'wb') as handle:\n",
    "    pickle.dump(author2doc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author2doc = pickle.load(open('author2doc.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author2doc_1={}\n",
    "for k, v in author2doc.items():\n",
    "    k_1 = re.sub('\\s', '', k)\n",
    "    k_1 = k_1.lower()\n",
    "    author2doc_1[k_1] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS datast, to an integer ID.\n",
    "doc_ids = list(set(papers['id']))\n",
    "doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "# Replace NIPS IDs by integer IDs.\n",
    "for a, a_doc_ids in author2doc_1.items():\n",
    "    for i, doc_id in enumerate(a_doc_ids):\n",
    "        author2doc_1[a][i] = doc_id_dict[doc_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 17.7 ms, total: 2.73 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "#docs_1 = docs\n",
    "STOPWORDS = ['(a)','(b)','(c)','(d)','(e)','(f)','(g)','(h)','(1)','(2)','(3)','(4)','(5)',\\\n",
    "             '(i)','(ii)','(iii)','(iiii)','(v)']\n",
    "for doc in nlp.pipe(paper_title, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities.\n",
    "    \n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords and remove single letter, e.g.,'x','y'.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop and len(token)>1]\n",
    "    \n",
    "     \n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in STOPWORDS]\n",
    "    #doc = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', doc)\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "docs = processed_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.     \n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ =dictionary[1]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 8595\n",
      "Number of unique tokens: 411\n",
      "Number of documents: 6560\n"
     ]
    }
   ],
   "source": [
    "print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                     author2doc = author2doc_1,\\\n",
    "                    chunksize=200,\\\n",
    "                    passes=100, gamma_threshold=1e-10,\\\n",
    "                    eval_every=0, iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 34s, sys: 4.19 s, total: 12min 38s\n",
      "Wall time: 12min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                     author2doc = author2doc_1,\\\n",
    "                    chunksize=100,\\\n",
    "                    passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)\n",
    "# Save model.\n",
    "model.save('/tmp/model.atmodel')\n",
    "# Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.086*\"analysis\" + 0.081*\"data\" + 0.052*\"high\" + 0.050*\"matrix\" + 0.045*\"models\" + 0.040*\"order\" + 0.039*\"clustering\" + 0.038*\"distributed\" + 0.036*\"dimensional\" + 0.036*\"structured\"'),\n",
       " (1,\n",
       "  '0.184*\"learn\" + 0.106*\"stochastic\" + 0.070*\"sparse\" + 0.058*\"gradient\" + 0.038*\"random\" + 0.034*\"descent\" + 0.030*\"temporal\" + 0.024*\"problem\" + 0.023*\"active\" + 0.022*\"search\"'),\n",
       " (2,\n",
       "  '0.093*\"linear\" + 0.080*\"efficient\" + 0.050*\"learning\" + 0.041*\"control\" + 0.034*\"machine\" + 0.031*\"local\" + 0.029*\"matching\" + 0.028*\"regularization\" + 0.025*\"distribution\" + 0.025*\"classification\"'),\n",
       " (3,\n",
       "  '0.151*\"neural\" + 0.151*\"network\" + 0.068*\"estimation\" + 0.063*\"non\" + 0.050*\"dynamic\" + 0.048*\"online\" + 0.048*\"networks\" + 0.027*\"latent\" + 0.026*\"submodular\" + 0.021*\"net\"'),\n",
       " (4,\n",
       "  '0.299*\"learning\" + 0.056*\"base\" + 0.046*\"function\" + 0.044*\"reinforcement\" + 0.042*\"multi\" + 0.027*\"graph\" + 0.027*\"application\" + 0.025*\"spectral\" + 0.020*\"algorithm\" + 0.020*\"inference\"'),\n",
       " (5,\n",
       "  '0.100*\"optimization\" + 0.059*\"method\" + 0.052*\"algorithms\" + 0.052*\"process\" + 0.048*\"gaussian\" + 0.047*\"feature\" + 0.033*\"selection\" + 0.030*\"parallel\" + 0.028*\"convergence\" + 0.027*\"fast\"'),\n",
       " (6,\n",
       "  '0.075*\"deep\" + 0.060*\"recognition\" + 0.046*\"prediction\" + 0.041*\"probabilistic\" + 0.040*\"information\" + 0.039*\"variational\" + 0.036*\"statistical\" + 0.035*\"object\" + 0.035*\"convolutional\" + 0.033*\"mixture\"'),\n",
       " (7,\n",
       "  '0.179*\"networks\" + 0.139*\"neural\" + 0.052*\"time\" + 0.048*\"model\" + 0.037*\"structure\" + 0.030*\"recurrent\" + 0.029*\"analog\" + 0.027*\"continuous\" + 0.025*\"state\" + 0.023*\"approach\"'),\n",
       " (8,\n",
       "  '0.086*\"bayesian\" + 0.084*\"models\" + 0.071*\"inference\" + 0.054*\"training\" + 0.048*\"large\" + 0.047*\"markov\" + 0.038*\"scale\" + 0.036*\"decision\" + 0.031*\"sampling\" + 0.029*\"adaptive\"'),\n",
       " (9,\n",
       "  '0.080*\"optimal\" + 0.048*\"visual\" + 0.045*\"convex\" + 0.042*\"model\" + 0.040*\"neuron\" + 0.034*\"sample\" + 0.031*\"bandit\" + 0.031*\"spike\" + 0.030*\"complexity\" + 0.030*\"regression\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuthorTopicModel.load('/tmp/model.atmodel')\n",
    "model.show_topics(num_topics=10, num_words=10, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic[0], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zhaoranwang\n",
      "Docs: [5572, 5209, 5690, 5877, 5970, 5871, 6439, 6463, 6051, 6475, 6263]\n",
      "Topics:\n",
      "[(1, 0.33019163573243099), (2, 0.22859663373031203), (9, 0.39167087869323086)]\n"
     ]
    }
   ],
   "source": [
    "show_author('zhaoranwang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##find document topics\n",
    "from sklearn import preprocessing\n",
    "def getdocumenttopics(corpus):\n",
    "    all_doc_topic=[]\n",
    "    all_doc_topic_prob=[]\n",
    "    for i in range(len(corpus)):\n",
    "        topicseq=[]\n",
    "        res = corpus[i]\n",
    "        lst=[]\n",
    "        for j in range(len(res)):\n",
    "            topic = res[j][0]            \n",
    "            count = res[j][1]\n",
    "            #get the possibility \n",
    "            if model.get_term_topics(topic):\n",
    "                a_topic =  model.get_term_topics(topic)[0][0]\n",
    "                pos = count* model.get_term_topics(topic)[0][1]\n",
    "                lst.append(pos)               \n",
    "                topicseq.append(a_topic) \n",
    "        #Scale to [0,1]  \n",
    "        lst_scaled= [item/np.sum(lst) for item in lst]\n",
    "        doc_topic_prob = list(zip(topicseq,lst_scaled))\n",
    "        all_doc_topic.append(topicseq)\n",
    "        all_doc_topic_prob.append(doc_topic_prob) \n",
    "    return all_doc_topic, all_doc_topic_prob\n",
    "\n",
    "all_topic, all_doc_topic_prob =getdocumenttopics(corpus)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "for i in range(len(all_doc_topic_prob)):\n",
    "    c = Counter()\n",
    "    for topic, prob in all_doc_topic_prob[i]:\n",
    "        c.update({topic: prob}) \n",
    "    all_doc_topic_prob[i] = list(c.items())\n",
    "for i in range(len(all_topic)):\n",
    "    all_topic[i]=list(set(all_topic[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findyearseq(doc_ids):\n",
    "    year = [] \n",
    "    for id in doc_ids:\n",
    "        for j,paper_id in enumerate(papers['id']):\n",
    "            if id==paper_id:\n",
    "                year.append(year_seq[j])\n",
    "    return year   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topicseq(name):\n",
    "    topics=[]\n",
    "    for i,doc_id in enumerate(model.author2doc[name]):  \n",
    "        # each document we choose one topic\n",
    "        doc_topics = all_topic[doc_id]\n",
    "        topics.append(doc_topics) \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trends of research direction\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "def trendofresearch(name):\n",
    "    # past current research direction\n",
    "    print(show_author(name))\n",
    "    doc_ids = model.author2doc[name]\n",
    "    topics = topicseq(name)\n",
    "    #print(topics)\n",
    "    length = len(topics)\n",
    "    print('length:',length)\n",
    "    flat_list = [item for sublist in topics for item in sublist]\n",
    "    topic_dict = {x:flat_list.count(x) for x in set(flat_list)}\n",
    "    print('topic_dict:',topic_dict)\n",
    "    major_topic=[]\n",
    "    for topic, count in topic_dict.items():\n",
    "        if count/length >0.5:\n",
    "            major_topic.append(topic)\n",
    "    if len(major_topic)>1:\n",
    "        print('This author works on variety topics: ', major_topic)\n",
    "    if len(major_topic)==1:\n",
    "        print('This author focus on one topic',major_topic)\n",
    "    if len(major_topic)==0:\n",
    "        print('This author changes topics frequentyly', major_topic)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zhaoranwang\n",
      "Docs: [5572, 5209, 5690, 5877, 5970, 5871, 6439, 6463, 6051, 6475, 6263]\n",
      "Topics:\n",
      "[(1, 0.33019163573243099), (2, 0.22859663373031203), (9, 0.39167087869323086)]\n",
      "None\n",
      "length: 11\n",
      "topic_dict: {0: 7, 1: 1, 2: 3, 3: 1, 4: 2, 5: 2, 6: 1, 7: 3, 8: 2, 9: 3}\n",
      "This author focus on one topic [0]\n"
     ]
    }
   ],
   "source": [
    "trendofresearch('zhaoranwang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plot pie chart or radar chart to visualize the distribution of the topics for each author    \n",
    "\n",
    "import pygal\n",
    "def visualizetopicdistribution(name):\n",
    "    radar_chart = pygal.Radar()\n",
    "    radar_chart.title = 'Topic distribution For Author' + name\n",
    "    res = model[name]\n",
    "    topic_seq = ['topic' + ' ' + str(i) for i in range(10)]\n",
    "\n",
    "    topic_prob = np.zeros(10)\n",
    "    for i in range(len(res)):\n",
    "        topic = res[i][0]\n",
    "        prob = res[i][1]\n",
    "        topic_prob[topic] = prob\n",
    "    doc_ids = model.author2doc[name]\n",
    "    topics = topicseq(name)\n",
    "    length = len(topics)\n",
    "\n",
    "    flat_list = [item for sublist in topics for item in sublist]\n",
    "    topic_dict = {x:flat_list.count(x) for x in set(flat_list)}\n",
    "    topic_prob_1 = np.zeros(10)\n",
    "    for topic, value in topic_dict.items():\n",
    "        topic_prob_1[topic]=value/sum(topic_dict.values())\n",
    "\n",
    "\n",
    "    radar_chart.x_labels = topic_seq\n",
    "    radar_chart.add('Topics proabablity by ATM', topic_prob)\n",
    "    radar_chart.add('Topics proabablity by Frequency', topic_prob_1)\n",
    "    radar_chart.render_to_file('radar_chart.svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizetopicdistribution('zhaoranwang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
